---
title: go compile lexer　词法分析
date: 2024-09-03
private: true
---
# Preface

# lex
> 本方涉及的源码见 ./code/
> 早期的 Go 语言虽然使用 lex 这种工具来生成词法解析器，但是最后还是使用 Go 来实现词法分析器，用自己写的词法分析器来解析自己8。
lex 是用于生成词法分析器的工具.  它是一个通用的正则生成器，将代码翻译成token. 通用于各类语言。

## lex 规则
一个lex　文件示例，它可以用来匹配简单的go package/import/brace/Quotes 关键字

    $ cat simplego.l
    %{
    #include <stdio.h>
    %}

    %%
    package      printf("PACKAGE ");
    import       printf("IMPORT ");
    \.           printf("DOT ");
    \{           printf("LBRACE ");
    \}           printf("RBRACE ");
    \(           printf("LPAREN ");
    \)           printf("RPAREN ");
    \"           printf("QUOTE ");
    \n           printf("\n");
    [0-9]+       printf("NUMBER ");
    [a-zA-Z_]+   printf("IDENT ");
    %%

## 编译lex 规则
lex 规则是不用运行的。需要用lex命令编译生成`*.yy.c`(flex包提供, mac　应该是自带了)

    # brew instal flex 或用自带的
    $ lex simplego.l
    $ cat lex.yy.c
    ...
    int yylex (void) {
        ...
        while ( 1 ) {
            ...
            yy_match:
                    do {
                        register YY_CHAR yy_c = yy_ec[YY_SC_TO_UI(*yy_cp)];
                        if ( yy_accept[yy_current_state] ) {
                            (yy_last_accepting_state) = yy_current_state;
                            (yy_last_accepting_cpos) = yy_cp;
                        }
                        while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state ) {
                            yy_current_state = (int) yy_def[yy_current_state];
                            if ( yy_current_state >= 30 )
                                yy_c = yy_meta[(unsigned int) yy_c];
                            }
                        yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
                        ++yy_cp;
                    } while ( yy_base[yy_current_state] != 37 );
                    ...
            do_action:
                    switch ( yy_act )
                        case 1:
                            YY_RULE_SETUP
                            printf("PACKAGE ");
                            YY_BREAK

lex.yy.c4 的前 600 行基本都是宏和函数的声明和定义，后面生成的代码大都是为 yylex 这个函数服务的
这个yylex 函数使用有限自动机（Deterministic Finite Automaton、DFA）的程序结构来分析输入的字符流， 上述代码中 while 循环就是这个有限自动机的主体.

当前的文件中并不存在 main 函数，main 函数是在 liblex 库中定义的，所以在编译时其实需要添加额外的 -ll 选项：

    $ cc lex.yy.c -o simplego.bin -ll
    $ cat main.go | ./simplego.bin
    PACKAGE  IDENT 
    IMPORT  QUOTE IDENT QUOTE 
    ...

# go词法分析(syntax.scanner)
scanner: Go 语言的词法解析是通过 src/cmd/compile/internal/syntax/scanner.go 文件中的 scanner 结构体实现的

    // 这个结构体会持有当前扫描的数据源文件、启用的模式和当前被扫描到的 Token
    type scanner struct {
        source
        mode   uint
        nlsemi bool

        // current token, valid after calling next()
        line, col uint
        blank     bool // line is blank up to col
        tok       token
        lit       string   // valid if tok is _Name, _Literal, or _Semi ("semicolon", "newline", or "EOF"); may be malformed if bad is true
        bad       bool     // valid if tok is _Literal, true if a syntax error occurred, lit may be malformed
        kind      LitKind  // valid if tok is _Literal
        op        Operator // valid if tok is _Operator, _AssignOp, or _IncOp
        prec      int      // valid if tok is _Operator, _AssignOp, or _IncOp
    }

go token 可分成几个不同的类别: 名称和字面量、操作符、分隔符和关键字。
src/cmd/compile/internal/syntax/tokens.go 文件中定义了 Go 语言中支持的全部 Token 类型，

    const (
        _    token = iota
        _EOF       // EOF

        // operators and operations
        _Operator // op
        ...

        // delimiters
        _Lparen    // (
        _Lbrack    // [
        ...

        // keywords
        _Break       // break
        ...
        _Type        // type
        _Var         // var

    )

词法分析主要是由 cmd/compile/internal/syntax.scanner 这个结构体中的 syntax.scanner.next 方法驱动，这个 250 行函数的主体是一个 switch/case 结构：

    func (s *scanner) next() {
        ...
        s.stop()
        startLine, startCol := s.pos()
        for s.ch == ' ' || s.ch == '\t' || s.ch == '\n' && !nlsemi || s.ch == '\r' {
            s.nextch()
        }

        s.line, s.col = s.pos()
        s.blank = s.line > startLine || startCol == colbase
        s.start()
        if isLetter(s.ch) || s.ch >= utf8.RuneSelf && s.atIdentChar(true) {
            s.nextch()
            s.ident()
            return
        }

        switch s.ch {
        case -1:
            s.tok = _EOF

        case '0', '1', '2', '3', '4', '5', '6', '7', '8', '9':
            s.number(false)
        ...
        }
    }

cmd/compile/internal/syntax.scanner 每次都会通过 cmd/compile/internal/syntax.source.nextch 函数获取文件中最近的未被解析的字符，然后根据当前字符的不同执行不同的 case.

1.如果遇到了空格和换行符这些空白字符会直接跳过
2.如果当前字符是 0 就会执行 cmd/compile/internal/syntax.scanner.number 方法尝试匹配一个数字。

    // 分析匹配浮点数、指数和复数
    func (s *scanner) number(seenPoint bool) {
        kind := IntLit
        base := 10        // number base
        digsep := 0
        invalid := -1     // index of invalid digit in literal, or < 0

        s.kind = IntLit
        if !seenPoint {
            digsep |= s.digits(base, &invalid)
        }

        s.setLit(kind, ok)
    }

    func (s *scanner) digits(base int, invalid *int) (digsep int) {
        max := rune('0' + base)
        for isDecimal(s.ch) || s.ch == '_' {
            ds := 1
            if s.ch == '_' {
                ds = 2
            } else if s.ch >= max && *invalid < 0 {
                _, col := s.pos()
                *invalid = int(col - s.col) // record invalid rune index
            }
            digsep |= ds
            s.nextch()
        }
        return
    }

词法分析匹配整数的逻辑：
1. 在 for 循环中不断获取最新的字符，将字符通过 syntax.source.nextch 方法追加到 syntax.scanner 持有的缓冲区中；
2. 当前包中的词法分析器 syntax.scanner 也只是为上层提供了 syntax.scanner.next 方法

# 语法分析(syntax.parser)
语法分析是根据某种特定的形式文法（Grammar）对 Token 序列构成的输入文本进行分析并确定其语法结构的过程。

先了解语法分析中的文法和分析方法。

## 文法
上下文无关文法是用来形式化、精确描述某种编程语言的工具，它包含一系列用于转换字符串的生产规则（Production rule）。

每一个生产规则(有限状态机)都会将规则`左侧的非终结符`转换成`右侧的字符串`，文法都由以下的四个部分组成：

    // tip: 终结符是文法中无法再被展开的符号，而非终结符与之相反

    𝑁 有限个非终结符的集合；
    Σ 有限个终结符的集合；
    𝑃 有限个生产规则的集合；
    𝑆 非终结符集合中唯一的开始符号；

文法被定义成一个四元组 `(𝑁,Σ,𝑃,𝑆)` ，这个元组中的几部分是上面提到的四个符号，其中每个生产规则P 都会包含非终结符、终结符或者开始符号，我们在这里可以举个简单的例子：

    𝑆→𝑎𝑆𝑏
    𝑆→𝑎𝑏
    𝑆→𝜖

上述规则构成的文法就能够表示 ab、aabb 以及 aaa..bbb 等字符串，编程语言的文法就是由这一系列的生产规则表示的，在这里我们可以从 src/cmd/compile/internal/syntax/parser.go13 文件中摘抄一些 Go 语言文法的生产规则：

    SourceFile = PackageClause ";" { ImportDecl ";" } { TopLevelDecl ";" } .
    PackageClause  = "package" PackageName .
    PackageName    = identifier .

    ImportDecl       = "import" ( ImportSpec | "(" { ImportSpec ";" } ")" ) .
    ImportSpec       = [ "." | PackageName ] ImportPath .
    ImportPath       = string_lit .

    TopLevelDecl  = Declaration | FunctionDecl | MethodDecl .
    Declaration   = ConstDecl | TypeDecl | VarDecl .

> Go 语言更详细的文法可以从 Language Specification14 中找到，这里不仅包含语言的文法，还包含词法元素、内置函数等信息。

因为每个 Go 源代码文件最终都会被解析成一个独立的抽象语法树，所以语法树最顶层的结构或者开始符号都是 SourceFile：

    SourceFile = PackageClause ";" { ImportDecl ";" } { TopLevelDecl ";" } .

从 SourceFile 相关的生产规则我们可以看出，每一个文件都包含一个 package 的定义以及可选的 import 声明和其他的顶层声明（TopLevelDecl），每一个 SourceFile 在编译器中都对应一个 cmd/compile/internal/syntax.File 结构体，你能从它们的定义中轻松找到两者的联系：

    type File struct {
        Pragma   Pragma
        PkgName  *Name
        DeclList []Decl
        Lines    uint
        node
    }

顶层声明有五大类型，分别是常量、类型、变量、函数和方法，你可以在文件 src/cmd/compile/internal/syntax/parser.go 中找到这五大类型的定义。
下面的文法分别定义了 Go 语言中常量、类型和变量三种常见的结构:

    ConstDecl = "const" ( ConstSpec | "(" { ConstSpec ";" } ")" ) .
    ConstSpec = IdentifierList [ [ Type ] "=" ExpressionList ] .

    TypeDecl  = "type" ( TypeSpec | "(" { TypeSpec ";" } ")" ) .
    TypeSpec  = AliasDecl | TypeDef .
    AliasDecl = identifier "=" Type .
    TypeDef   = identifier Type .

    VarDecl = "var" ( VarSpec | "(" { VarSpec ";" } ")" ) .
    VarSpec = IdentifierList ( Type [ "=" ExpressionList ] | "=" ExpressionList ) .

函数和方法的定义就更加复杂，从下面的文法我们可以看到 Statement 总共可以转换成 15 种不同的语法结构，这些语法结构就包括我们经常使用的 switch/case、if/else、for 循环以及 select 等语句：

    FunctionDecl = "func" FunctionName Signature [ FunctionBody ] .
    FunctionName = identifier .
    FunctionBody = Block .

    MethodDecl = "func" Receiver MethodName Signature [ FunctionBody ] .
    Receiver   = Parameters .

    Block = "{" StatementList "}" .
    StatementList = { Statement ";" } .

    Statement =
        Declaration | LabeledStmt | SimpleStmt |
        GoStmt | ReturnStmt | BreakStmt | ContinueStmt | GotoStmt |
        FallthroughStmt | Block | IfStmt | SwitchStmt | SelectStmt | ForStmt |
        DeferStmt .

    SimpleStmt = EmptyStmt | ExpressionStmt | SendStmt | IncDecStmt | Assignment | ShortVarDecl .

> 对于 Statement 展开参考 src/cmd/compile/internal/syntax/parser.go 

## 分析方法
语法分析的分析方法一般分为自顶向下和自底向上对输入的 Token 序列进行推导：

1. 自顶向下分析：可以被看作找到当前输入流最左推导的过程，对于任意一个输入流，根据当前的输入符号，确定一个生产规则，使用生产规则右侧的符号替代相应的非终结符向下推导；
2. 自底向上分析：语法分析器从输入流开始，每次都尝试重写最右侧的多个符号，这其实是说解析器会从最简单的符号进行推导，在解析的最后合并成开始符号；

### 自顶向下
LL 文法是一种使用自顶向下分析方法的文法，下面给出了一个常见的 LL 文法：

    𝑆→𝑎𝑆1
    𝑆1→𝑏𝑆1
    𝑆1→𝜖

假设我们存在以上的生产规则和输入流 abb，如果这里使用自顶向下的方式进行语法分析，：

    𝑆
    （开始符号）
    𝑎𝑆1
    （规则 1)
    𝑎𝑏𝑆1
    （规则 2)
    𝑎𝑏𝑏𝑆1
    （规则 2)
    𝑎𝑏𝑏
    （规则 3)

这种分析方法一定会从开始符号分析，通过下一个即将入栈的符号判断如果对右侧非终结符（𝑆 或 𝑆1 ）进行展开，直到整个字符串中不存在任何的非终结符，整个解析过程才会结束。

### 自底向上
但是如果我们使用自底向上的方式对输入流进行分析时，处理过程就会完全不同了，常见的四种文法 LR(0)、SLR、LR(1) 和 LALR(1) 使用了自底向上的处理方式，我们可以简单写一个与上一节中效果相同的 LR(0) 文法：

    𝑆→𝑆1
    𝑆1→𝑆1𝑏
    𝑆1→𝑎

使用上述等效的文法处理同样地输入流 abb 会使用完全不同的过程对输入流进行展开(逆向匹配)：

    𝑎
    （入栈）
    𝑆1
    （规则 3）
    𝑆1𝑏
    （入栈）
    𝑆1
    （规则 2）
    𝑆1𝑏
    （入栈）
    𝑆1
    （规则 2）
    𝑆
    （规则 1）

自底向上的分析过程会维护一个栈用于存储未被归约的符号，在整个过程中会执行两种不同的操作，一种叫做入栈（Shift），也就是将下一个符号入栈，另一种叫做归约（Reduce），也就是对最右侧的字符串按照生产规则进行合并。

上述的分析过程和自顶向下的分析方法完全不同，这两种不同的分析方法其实也代表了计算机科学中两种不同的思想 — 从抽象到具体和从具体到抽象。

### Lookahead
在语法分析中除了 LL 和 LR 这两种不同类型的语法分析方法之外，还存在另一个非常重要的概念，就是向前查看（Lookahead），在不同生产规则发生冲突时，当前解析器需要通过预读一些 Token 判断当前应该用什么生产规则对输入流进行展开或者归约，例如在 LALR(1) 文法中，需要预读一个 Token 保证出现冲突的生产规则能够被正确处理。

## go语法分析(syntax.parser)
> Go 语言的解析器使用了 LALR(1) 的文法来解析词法分析过程中输出的 Token 序列，最右推导加向前查看构成了 Go 语言解析器的最基本原理，也是大多数编程语言的选择。

我们在概述中已经介绍了编译器的主函数:
1. 该函数调用的 cmd/compile/internal/gc.parseFiles 会使用多个 Goroutine 来解析源文件，
2. 解析的过程会调用 cmd/compile/internal/syntax.Parse，
3. 该函数初始化了一个新的 cmd/compile/internal/syntax.parser 结构体
   1. 并通过 cmd/compile/internal/syntax.parser.fileOrNil 方法开启对当前文件的词法和语法解析：

### fileOrNil 分析
示例：

    func Parse(base *PosBase, src io.Reader, errh ErrorHandler, pragh PragmaHandler, mode Mode) (_ *File, first error) {
        var p parser
        p.init(base, src, errh, pragh, mode)
        p.next()
        return p.fileOrNil(), p.first
    }

#### fileOrNil 分析入口
cmd/compile/internal/syntax.parser.fileOrNil 方法其实是对上面介绍的 Go 语言文法的实现，该方法首先会解析文件开头的 package 定义：

    // SourceFile = PackageClause ";" { ImportDecl ";" } { TopLevelDecl ";" } .
    func (p *parser) fileOrNil() *File {
        f := new(File)
        f.pos = p.pos()

        if !p.got(_Package) {
            p.syntaxError("package statement must be first")
            return nil
        }
        f.PkgName = p.name()
        p.want(_Semi)

从上面的这一段方法中我们可以看出，当前方法会通过 cmd/compile/internal/syntax.parser.got 来判断下一个 Token 是不是 package 关键字，如果是 package 关键字，就会执行 cmd/compile/internal/syntax.parser.name 来匹配一个包名并将结果保存到返回的文件结构体中。

	for p.got(_Import) {
		f.DeclList = p.appendGroup(f.DeclList, p.importDecl)
		p.want(_Semi)
	}

确定了当前文件的包名之后，就开始解析可选的 import 声明，每一个 import 在解析器看来都是一个声明语句，这些声明语句都会被加入到文件的 DeclList 中。

在这之后会根据编译器获取的关键字进入 switch 的不同分支，这些分支调用 cmd/compile/internal/syntax.parser.appendGroup 方法并在方法中传入用于处理对应类型语句的 cmd/compile/internal/syntax.parser.constDecl、cmd/compile/internal/syntax.parser.typeDecl 函数。

	for p.tok != _EOF {
		switch p.tok {
		case _Const:
			p.next()
			f.DeclList = p.appendGroup(f.DeclList, p.constDecl)

		case _Type:
			p.next()
			f.DeclList = p.appendGroup(f.DeclList, p.typeDecl)

		case _Var:
			p.next()
			f.DeclList = p.appendGroup(f.DeclList, p.varDecl)

		case _Func:
			p.next()
			if d := p.funcDeclOrNil(); d != nil {
				f.DeclList = append(f.DeclList, d)
			}
		default:
			...
		}
	}

	f.Lines = p.source.line

	return f

cmd/compile/internal/syntax.parser.fileOrNil 使用了非常多的子方法对输入的文件进行语法分析，并在最后会返回文件开始创建的 cmd/compile/internal/syntax.File 结构体。

#### parser 与　scanner
读到这里的人可能会有一些疑惑，为什么没有看到词法分析的代码，这是因为:
1. 因为词法分析器 cmd/compile/internal/syntax.scanner 作为结构体被嵌入到了 cmd/compile/internal/syntax.parser 中，
2. 所以这个方法中的 p.next() 实际上调用的是 cmd/compile/internal/syntax.scanner.next 方法，它会直接获取文件中的下一个 Token，所以词法和语法分析一起进行的。

#### fileOrNil 子树
cmd/compile/internal/syntax.parser.fileOrNil 与在这个方法中执行的其他子方法共同构成了一棵树:
1. 这棵树根节点是 cmd/compile/internal/syntax.parser.fileOrNil，
2. 子节点是 cmd/compile/internal/syntax.parser.importDecl、cmd/compile/internal/syntax.parser.constDecl 等方法，它们与 Go 语言文法中的生产规则一一对应。

e.g.

    fileOrNil:
        constDecl
        varDecl
        importDecl
        TypeDecl
        funcDeclOrNil: (name + funcType + funcBody)

#### 生产规则
cmd/compile/internal/syntax.parser.fileOrNil、cmd/compile/internal/syntax.parser.constDecl 等方法对应了 Go 语言中的生产规则，例如 cmd/compile/internal/syntax.parser.fileOrNil 实现的是：

    SourceFile = PackageClause ";" { ImportDecl ";" } { TopLevelDecl ";" } .

通过将编程语言的所有生产规则映射到对应的方法上，这些方法构成的树形结构最终会返回一个抽象语法树。

> 这里只讲了syntax.parser.fileOrNil 方法的实现了，其它参考 src/cmd/compile/internal/syntax/parser.go (包含了语法分析阶段的全部方法)

### 辅助方法
这里讲一下解析器运行过程中有几个辅助方法

#### got+want
首先就是 cmd/compile/internal/syntax.parser.got 和 cmd/compile/internal/syntax.parser.want 这两个常见的方法：

    // syntax.parser.got 只是用于快速判断一些语句中的关键字，如果当前解析器中的 Token 是传入的 Token 就会直接跳过该 Token 并返回 true；
    func (p *parser) got(tok token) bool {
        if p.tok == tok {
            p.next()
            return true
        }
        return false
    }

    // syntax.parser.want 就是对 syntax.parser.got 的简单封装了
    func (p *parser) want(tok token) {
        if !p.got(tok) {
            p.syntaxError("expecting " + tokstring(tok))
            p.advance()
        }
    }

#### appendGroup
另一个方法 cmd/compile/internal/synctax.parser.appendGroup 的实现就稍微复杂了一点，它的主要作用就是找出批量的定义，举一个例子：

    var (
        a int
        b int
    )

这两个变量其实属于同一个组（Group），各种顶层定义的结构体 cmd/compile/internal/syntax.parser.constDecl、cmd/compile/internal/syntax.parser.varDecl 在进行语法分析时有一个额外的参数 cmd/compile/internal/syntax.Group，这个参数是通过 appendGroup 方法传递进去的：

    func (p *parser) appendGroup(list []Decl, f func(*Group) Decl) []Decl {
        if p.tok == _Lparen {
            g := new(Group)
            p.list(_Lparen, _Semi, _Rparen, func() bool {
                list = append(list, f(g))
                return false
            })
        } else {
            list = append(list, f(nil))
        }

        return list
    }

syntax.parser.appendGroup 方法会调用传入的 f 方法对输入流进行匹配并将匹配的结果追加到另一个参数 syntax.File 结构体中的 DeclList 数组中，
import、const、var、type 和 func 声明语句都是调用 cmd/compile/internal/syntax.parser.appendGroup 方法解析的。

### 节点 
语法分析器最终会使用不同的结构体来构建抽象语法树中的节点，其中根节点 cmd/compile/internal/syntax.File 包含了当前文件的包名、所有声明结构的列表和文件的行数：

    type File struct {
        Pragma   Pragma // 于存储与编译器指令（pragma）相关的信息
        PkgName  *Name
        DeclList []Decl // 变量、常量、类型、函数 等声明
        Lines    uint
        node
    }

src/cmd/compile/internal/syntax/nodes.go 文件中也定义了其他节点的结构体，其中包含全部声明类型的

    type (
        Decl interface {
            Node
            aDecl()
        }

        FuncDecl struct {
            Attr   map[string]bool
            Recv   *Field
            Name   *Name
            Type   *FuncType
            Body   *BlockStmt // 函数体块
            Pragma Pragma
            decl
        }
    )

其实BlockStmt 结构体其实是`{List []Stmt, stmt}`, 也就是cmd/compile/internal/syntax.Stmt 接口. 

实现该接口的类型其实也非常多，总共有 14 种不同类型的 syntax.Stmt 实现：

    // /src/cmd/compile/internal/syntax/nodes.go#L317
    BlockStmt, LabeledStmt
    IfStmt、ForStmt、SwitchStmt 和 SelectStmt
    CallStmt, ReturnStmt
    ExprStmt
    ...
