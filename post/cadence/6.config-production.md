---
title: cadence config production
date: 2021-12-01
private: true
---
# cadence config
refer: https://cadenceworkflow.io/docs/operation-guide/setup/#static-configuration
1. You should understand some basic `static configuration` of Cadence cluster.
2. There are also many other configuration called "`Dynamic Configuration`" for fine tuning the cluster(微调集群). The default values are good to go for small clusters.
3. Cadence’s `minimum dependency` is a database(Cassandra or SQL based like MySQL/Postgres). 
4. For **production** you also need a `metric server(Prometheus/Statsd/M3/etc)`.
5. For **advanced features** Cadence depends on others like `ElastiCache+Kafka` if you need `Advanced visibility feature` to search workflows. 
5. Cadence will depends on a blob store like `S3` if you need to `enable archival feature`

## static configuration
conig/development.yaml

## dynamic configuration
There are a lot more dynamic configurations than static configurations. Most of the default values are good for small clusters. As a cluster is scaled up, you may look for tuning it for the optimal performance.


For example, search for "EnableGlobalDomain" in Dynamic Configuration Comments in v0.21.0 

1. KeyName is the key that you will use in the dynamicconfig yaml content
1. Default value is the default value
2. Value type indicates the type that you should change the yaml value of
4. Allowed filters indicates what kinds of filters you can set as constraints with the dynamic configuration.
    1. `DomainName` can be used with `domainName`
    2. `N/A` means no filters can be set. The config will be global.

For example, if you want to change the ratelimiting for List API, below is the config:

    // FrontendVisibilityListMaxQPS is max qps frontend can list open/close workflows
    // KeyName: frontend.visibilityListMaxQPS
    // Value type: Int
    // Default value: 10
    // Allowed filters: DomainName
    FrontendVisibilityListMaxQPS
    
Then you can add the config like:

    frontend.visibilityListMaxQPS:
      - value: 1000
      constraints:
        domainName: "domainA"
      - value: 2000
      constraints:
        domainName: "domainB"      
    
You will expect to see `domainA` will be able to perform `1K List operation per second`, while `domainB` can perform `2K per second`.

NOTE 2: for `<frontend,history,matching>.persistenceMaxQPS` versus `<frontend,history,matching>.persistenceGlobalMaxQPS` 
1. persistenceMaxQPS is local for single node 
2. while persistenceGlobalMaxQPS is global for all node. persistenceGlobalMaxQPS is preferred if set as greater than zero. But by default it is zero so persistenceMaxQPS is being used.

### How to update Dynamic Configuration
#### docker update
方法1： Local docker-compose by mounting volume: 

    cadence:
      image: ubercadence/server:master-auto-setup
      ports:
        ...(don't change anything here)
      environment:
        ...(don't change anything here)
        - "DYNAMIC_CONFIG_FILE_PATH=/etc/custom-dynamicconfig/development.yaml"
      volumes:
        - "/Users/<?>/cadence/config/dynamicconfig:/etc/custom-dynamicconfig"
 
方法2： Local docker-compose by logging into the container: 
1. run `docker exec -it docker_cadence_1 /bin/bash` to login your container. Then `vi config/dynamicconfig/development.yaml` to make any change. 
2. run `docker restart docker_cadence_1` to restart the cadence instance. 
3. Note that you can also use this approach to change static config, but it must be changed through `config/config_template.yaml` instead of `config/docker.yaml` because `config/docker.yaml` is generated on startup.

#### helm update
In production cluster: Follow this example of Helm Chart to deploy Cadence, update dynamic config here (https://github.com/banzaicloud/banzai-charts/blob/be57e81c107fd2ccdfc6cf95dccf6cbab226920c/cadence/templates/server-configmap.yaml#L170)
and restart the cluster.

#### DEBUG: How to make sure your updates on dynamicconfig is loaded?
for example, if you added the following to development.yaml

    frontend.visibilityListMaxQPS:
      - value: 10000
    
After `restarting Cadence instances`, execute a command like this to let Cadence load the config(it's lazy loading when using it).\

    $ cadence --domain <> workflow list
    # Then you should see the logs like below
    cadence_1        | {"level":"info","ts":"2021-05-07T18:43:07.869Z","msg":"First loading dynamic config","service":"cadence-frontend","key":"frontend.visibilityListMaxQPS,domainName:sample,clusterName:primary","value":"10000","default-value":"10","logging-call-at":"config.go:93"}
    
## Other Advanced Features
todo


# Cluster Maintenance
https://cadenceworkflow.io/docs/operation-guide/maintain/#scale-up-down-cluster

## Scale up & down Cluster
1. When CPU/Memory is getting bottleneck on Cadence instances, you may scale up or add more instances.
1. Watch Cadence metrics
    2. See if the external traffic to frontend is normal
    2. If the slowness is due to too many tasks on a tasklist, you may need to scale up the tasklist
    2. If persistence latency is getting too high, try scale up your DB instance
3. Never change the `numOfShards` of a cluster. 

## Scale up a tasklist using Scalable tasklist feature
By default a tasklist is not scalable enough to support `hundreds of tasks per second`. That’s mainly because each tasklist is assigned to a Matching service node, and dispatching tasks in a tasklist is in sequence.

In the past, Cadence recommended using multiple tasklists to start workflow/activity. You need to make a list of tasklists and randomly pick one when starting workflows. And then when starting workers, let them listen to all the tasklists.

Nowadays, Cadence has a feature called “Scalable tasklist”. It will divide a tasklist into multiple logical partitions, which can distribute tasks to multiple Matching service nodes. By default this feature is not enabled because there is some performance penalty on the server side, plus it’s not common that a tasklist needs to support more than hundreds tasks per second.

You must make a `dynamic configuration` change in Cadence server to use this feature:

    matching.numTasklistWritePartitions
    matching.numTasklistReadPartitions

There are a few things to know when using this feature:

1. Always make sure `matching.numTasklistWritePartitions <= matching.numTasklistReadPartitions`
2. Because of above, when scaling down the number of partitions, you must decrease the WritePartitions first, to wait for a certain time to ensure that tasks are drained, and then decrease ReadPartitions.
3. Both domain names and taskListName should be specified in the dynamic config. 

An example of using this feature.

    matching.numTasklistWritePartitions:
      - value: 10
        constraints:
          domainName: "samples-domain"
          taskListName: "aScalableTasklistName"
    matching.numTasklistReadPartitions:
      - value: 10
        constraints:
          domainName: "samples-domain"
          taskListName: "aScalableTasklistName"
## Optimize SQL Persistence
1. `Frontend and history nodes` need both `default and visibility Stores`, but `matching and sys workers` only need default Stores, they don't need to talk to visibility DBs.
2. For **default Stores**, `history service` will take the most connection, then `Frontend/Matching`. `SysWorker` will use much less than others
3. **Default Stores** is for Cadence’ core data model, which requires `strong consistency`. So it `cannot use replicas`. **VisibilityStore** is not for core data models. It’s recommended to use a separate DB for visibility store if using DB based visibility.
4. Visibility Stores usually take much less connection as the workload is much lightweight(less QPS and no explicit transactions).
5. Visibility Stores require `eventual consistency for read`. So it can use `replicas`.
6. MaxIdelConns should be less than MaxConns, so that the connections can be distributed better across hosts.

## Upgrading Server
