---
title: k8s intro
date: 2019-05-16
private:
---

# book

参考书
1. video:
   1. https://www.bilibili.com/video/BV1Tg411P7EB?p=2&vd_source=c19c4980a244fedcc729762ff654bbc9
   1. https://k8s.easydoc.net/docs/dRiQjyTY/28366845/6GiNOzyZ/9EX8Cp45
2. https://lib.jimmysong.io/kubernetes-handbook/

# Kubernetes 构架

> 一个node 中有多个pod, 一个pod 中有多个containers(nginx/redis/golang/deno)
> ![k8s-arch2](/img/k8s/k8s-arch2.webp)

典型的 Kubernetes 集群包含一个 master 和很多 node(worker)。

1. Master 是控制集群的中心:
   1. etcd: etcd 用来协同和存储配置，分别是：
      1. 存储网络插件 flannel网络信息
      2. Kubernetes 本身，包括各种对象的状态和元信息配置
   2. API Server : 面向用户的 API 服务
   3. Scheduler : 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上；
   4. Controller : 负责维护集群状态, 比如故障检测、自动扩展、滚动更新
2. Node: 最小硬件节点, 提供 CPU、内存和存储资源的节点。(相当于虚拟机吧), 包括
   1. pod: 类似docker-compose, 可有多个pod, 是部署最小单元
      1. 每个pod可以有自己的虚拟ip
      2. Kubernetes 中部署的最小单位是 pod， 一个 pod 中可以包含一个或多个 Docker 容器. 除非紧密耦合通常一个 pod
         中只有一个容器
      3. docker-compose中的多个containers 放到一个pod中
   2. kubelet : 和master 通信，报告node状态
   3. kube-proxy: 实现集群网络服务的,kube-proxy 等同于 etcd 网关

Kubernetes 是不依赖于 Docker 的，完全可以使用其他的容器引擎在 Kubernetes 管理的集群中替代 Docker

## Kubernetes 核心组件

![k8s-arch1](/img/k8s/k8s-arch1.webp)

1. etcd 保存了整个集群的状态
2. apiserver 提供了资源操作的唯一入口: 认证、授权、访问控制、api注册和发现
3. controller manager 维护集群状态：比如故障检测、自动扩展、滚动更新
4. scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上；
5. kubelet 负责维护容器的生命周期，同时也负责 Volume（CSI）和网络（CNI）的管理；
6. Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）；
7. kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡；

除了核心组件，还有一些推荐的插件，其中有的已经成为 CNCF 中的托管项目：

1. CoreDNS 负责为整个集群提供 DNS 服务
1. Ingress Controller 为服务提供外网入口
1. Prometheus 提供资源监控
1. Dashboard 提供 GUI
1. Federation 提供跨可用区的集群

## k8s 分层架构

1. 核心层(Nucleus API and excution)： Kubernetes 最核心的功能，对外提供 API
   构建高层的应用，对内提供插件式应用执行环境
1. 应用层(Application: deployment and routing)：
   部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等）、Service Mesh（部分位于应用层）
1. 管理层(Governance Layer: automation and policy enforcement)：
   系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision
   等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等）、Service Mesh（部分位于管理层）
1. 接口层(Interface Layer: client libraries and utilities)： kubectl 命令行工具、客户端 SDK
   以及集群联邦
1. 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴 
   1. Kubernetes 外部：日志、监控、配置管理、CI/CD、Workflow、FaaS、OTS 应用、ChatOps、GitOps、SecOps 等 
   2. Kubernetes 内部：CRI、CNI、CSI、镜像仓库、Cloud Provider、集群自身的配置和管理等

# etcd

整个 Kubernetes 系统中一共有两个服务需要用到 etcd 用来协同和存储配置，分别是：

1. 网络插件 flannel: 使用的是 v2 的 API
2. Kubernetes 本身，包括各种对象的状态和元信息配置: 使用的是 v3 的 API

## 查看k8s配置

Kubernetes 使用 etcd v3 的 API 操作 etcd 中的数据。所有的资源对象都保存在 /registry 路径下

查看所有Pod 信息

    ETCDCTL_API=3 etcdctl get /registry/pods --prefix -w json|python -m json.tool

# 开放接口

Kubernetes 中开放的以下接口，可以分别对接不同的后端，来实现自己的业务逻辑：

    容器运行时接口（CRI）：提供计算资源
        CRI 中定义了 容器 和 镜像 的服务的接口
    容器网络接口（CNI）：提供网络资源

    容器存储接口（CSI），提供存储资源

## 容器运行时接口（CRI）

容器运行时接口（Container Runtime Interface），简称 CRI

    架构：kubelet(grpc client) -> CRI shim(grpc server) -> container/image runtime

默认是启用了CRI，手动启用 只用在kubelet 加启动参数

    --container-runtime-endpoint unix:///var/run/dockershim.sock
    # 也可是tcp://
    --container-runtime-endpoint tcp://localhost:373

### 当前支持的 CRI 后端

默认使用 Docker 作为容器运行时, 通过 CRI 接口可以指定使用其它容器运行时作为 Pod 的后端, 目前支持

    Docker
    containerd # Kubernetes本身正在远离Docker而转向Containerd
    CRI-O   
        k8s 的标准实现
        CRI-O的优势在于其采用极简风格，或者说它的设计本身就是作为“CRI”运行时存在。不同于作为Docker组成部分的containerd，CRI-O在本质上属于纯CRI运行时、因此不包含除CRI之外的任何其他内容。
    Podman
    Buildah

## 容器网络接口（CNI）

Kubernetes 源码的 vendor/github.com/containernetworking/cni/libcni 目录中已经包含了 CNI 的代码
包括以下几个方法：

    type CNI interface {
        AddNetworkList (net *NetworkConfigList, rt *RuntimeConf) (types.Result, error)
        DelNetworkList (net *NetworkConfigList, rt *RuntimeConf) error
        AddNetwork (net *NetworkConfig, rt *RuntimeConf) (types.Result, error)
        DelNetwork (net *NetworkConfig, rt *RuntimeConf) error
    }

### 可用插件

Main：接口创建

    bridge：创建网桥，并添加主机和容器到该网桥
    ipvlan：在容器中添加一个 ipvlan 接口
    loopback：创建一个回环接口
    macvlan：创建一个新的 MAC 地址，将所有的流量转发到容器
    ptp：创建 veth 对
    vlan：分配一个 vlan 设备

IPAM：IP 地址分配

    dhcp：在主机上运行守护程序，代表容器发出 DHCP 请求
    host-local：维护分配 IP 的本地数据库

Meta：其它插件

    flannel：根据 flannel 的配置文件创建接口
    tuning：调整现有接口的 sysctl 参数
    portmap：一个基于 iptables 的 portmapping 插件。将端口从主机的地址空间映射到容器。

## 容器存储接口（CSI, Container Storage Interface）

https://lib.jimmysong.io/kubernetes-handbook/architecture/open-interfaces/csi/

您可以在任何的 pod 或者 pod 的 template 中引用你绑定到 CSI volume 上的
PersistentVolumeClaim(my-request-for-storage)。

    kind: Pod
    apiVersion: v1
    metadata:
      name: my-pod
    spec:
      containers:
        - name: my-frontend
          image: dockerfile/nginx
          volumeMounts:
          - mountPath: "/var/www/html"
            name: my-csi-volume
      volumes:
        - name: my-csi-volume
          persistentVolumeClaim:
            claimName: my-request-for-storage

# 生产环境

## Cgroup 驱动程序
控制组用来约束分配给进程的资源。

1. 当某个 Linux 系统发行版使用 systemd 作为其初始化系统时，初始化进程会生成并使用一个 root 控制组 (cgroup), 并充当
   cgroup 管理器。 Systemd 与 cgroup 集成紧密(systemd 带cgroup 驱动程序)，并将为每个 systemd 单元分配一个
   cgroup。 你也可以配置容器运行时和 kubelet 使用 cgroupfs。 连同 systemd 一起使用 cgroupfs 意味着将有两个不同的
   cgroup 管理器。

2. 单个 cgroup 管理器将简化分配资源的视图，并且默认情况下将对可用资源和使用 中的资源具有更一致的视图。
   当有两个管理器共存于一个系统中时，最终将对这些资源产生两种视图。 在此领域人们已经报告过一些案例，某些节点配置让 kubelet 和 docker 使用
   cgroupfs，而节点上运行的其余进程则使用 systemd; 这类节点在资源压力下 会变得不稳定。

3. 更改设置，令容器运行时和 kubelet 使用 systemd 作为 cgroup 驱动，以此使系统更为稳定。 对于 Docker, 设置
   native.cgroupdriver=systemd 选项。