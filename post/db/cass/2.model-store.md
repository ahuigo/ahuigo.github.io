---
title: cassandra model store
date: 2022-03-10
private: true
---
# 基本架构
> 此笔记为此文整理： Cassandra 原理介绍  https://www.cnblogs.com/logo-fox/p/8927067.html
Cassandra没有像BigTable或Hbase那样选择中心控制节点，而选择了无中心的P2P架构
1. 网络中的所有节点都是对等的，它们构成了一个环，
2. 节点之间通过P2P协议每秒钟交换一次数据，这样每个节点都拥有其它所有节点的信息，包括位置、状态等

客户端可以连接集群中的任一个节点，和客户端建立连接的节点叫协作者(coordinator)，它相当于一个代理，
1. 负责定位该次请求要发到哪些实际拥有本次请求所需数据的节点上去获取，但如何获取并返回，主要根据客户端要求的一致性级别（Consistency Level）来定
2. 比如：ONE指只要有一个节点返回数据就可以对客户端做出响应，QUONUM指需要返回几个根据用户的配置数目，ALL指等于数据复制份数的所有节点都返回结果才能向客户端做出响应，对于数据一致性要求不是特别高的可以选择ONE

Cassandra的核心组件包括：

1. Gossip：点对点的通讯协议，用来相互交换节点的位置和状态信息。当一个节点启动时就立即本地存储Gossip信息，但当节点信息发生变化时需要清洗历史信息，比如IP改变了。通过Gossip协议，每个节点定期每秒交换它自己和它已经交换过信息的节点的数据，每个被交换的信息都有一个版本号，这样当有新数据时可以覆盖老数据，为了保证数据交换的准确性，所有的节点必须使用同一份集群列表，这样的节点又被称作seed。

2. Partitioner：负责在集群中分配数据，由它来决定由哪些节点放置第一份的copy，一般情况会使用Hash来做主键，将每行数据分布到不同的节点上，以确保集群的可扩展性。

3. Replica placement strategy：复制策略，确定哪个节点放置复制数据，以及复制的份数。

3. Snitch：定义一个网络拓扑图，用来确定如何放置复制数据，高效地路由请求。
4. cassandra.yaml：主配置文件，设置集群的初始化配置、表的缓存参数、调优参数和资源使用、超时设定、客户端连接、备份和安全

# 存储结构
对于每一个column family(table)的数据一共有3个层次的数据存储：
1. commit log: 提交记录
2. memtable 内存: 会flush 到SSTable
3. SSTable（硬盘数据文件)它是Log-Structured Storage Table的简称。 


memtable和 SSTable 里存储数据时不存储列名: https://teddymaef.github.io/learncassandra/cn/model/where_is_data_stored.html
例如，如果用下面的CQL插入数据：

    INSERT INTO k1.t1 (c1) VALUES (v1);
    INSERT INTO k2.t1 (c1, c2) VALUES (v1, v2);
    INSERT INTO k1.t1 (c1, c3, c2) VALUES (v4, v3, v2);

在磁盘上的commit log里，Cassandra保存了下面的数据:

    k1, c1:v1
    k2, c1:v1 C2:v2
    k1, c1:v4 c3:v3 c2:v2

在memtable里, Cassandra保存了下面的数据:

    k1 c1:v4 c2:v2 c3:v3
    k2 c1:v1 c2:v2

在磁盘上的SSTable里，Cassandra在memtable被刷新的时候保存下面的数据：

    k1 c1:v4 c2:v2 c3:v3
    k2 c1:v1 c2:v2

## 写入
### 写入过程
当写事件发生时:
1. 首先由Commit Log捕获写事件并持久化: 当发生硬件故障时，用来自动重建memtable中还没有保存到SSTable的数据。
2. 之后数据也会被写入到内存中，叫Memtable
2. 当内存满了之后写入数据文件，叫SSTable
    1. 定期异步对SSTable做数据合并(Compaction)以减少数据读取时的查询时间。
    2. 因为写入操作只涉及到顺序写入和内存操作，因此有非常高的写入性能。而进行读操作时，Cassandra支持像LevelDB一样的实现机制，数据分层存储，将热点数据放在Memtable和相对小的SSTable中，所以能实现非常高的读性能。

### cass 节点同步：
1. 如果客户端配置了Consistency Level是ONE，意味着只要有一个节点写入成功，就由代理节点(Coordinator)返回给客户端写入完成。当然这中间有可能会出现其它节点写入失败的情况，Cassandra自己会通过Hinted Handoff或Read Repair 或者Anti-entropy Node Repair方式保证数据最终一致性。

2. Coordinator复制：对于多数据中心的写入请求，Cassandra做了优化，每个数据中心选取一个Coordinator来完成它所在数据中心的数据复制，这样客户端连接的节点只需要向数据中心的一个节点转发复制请求即可，由这个数据中心的Coordinator来完成该数据中心内的数据复制。

### LSM树
Cassandra的存储结构类似LSM树（Log-Structured Merge Tree）这种结构，不像传统数据一般都使用B+树，存储引擎以追加的方式顺序写入磁盘连续存储数据，写入是可以并发写入，不像B+树一样需要加锁，写入速度非常高，LevelDB、Hbase都是使用类似的存储结构。

1. LSM树 插入数据可以看作是一个N阶合并树: 数据写操作（包括插入、修改、删除也是写）都在内存中进行，
2. LSM读取性能较差，cass要定期异步对SSTable做数据合并(Compaction)以减少数据读取时的查询时间

不单指Insert操作，Update操作也是如此，Cassandra对Update操作的处理和传统关系数据库完全不一样，并不立即对原有数据进行更新，而是会增加一条新的记录，后续在进行Compaction时将数据再进行合并。
Delete操作也同样如此，要删除的数据会先标记为Tombstone，后续进行Compaction时再真正永久删除。

### Memtable 到 SSTable
Commit Log: 会在Memtable中的数据刷入SSTable后被清除掉，因此它不会占用太多磁盘空间，Cassandra的配置时也可以单独设置存储区(包括SSD,HHD)

写入到Memtable时: 
Cassandra能够动态地为它分配内存空间，你也可以使用工具自己调整。当达到阀值后，Memtable中的数据和索引会被放到一个队列中，然后flush到磁盘，可以使用memtableflushqueue_size参数来指定队列的长度。当进行flush时，会停止写请求。也可以使用nodetool flush工具手动刷新 数据到磁盘，重启节点之前最好进行此操作，以减少Commit Log回放的时间。为了刷新数据，会根据partition key对Memtables进行重排序，然后顺序写入磁盘。这个过程是非常快的，因为只包含Commit Log的追加和顺序的磁盘写入。

当memtable中的数据刷到SSTable后:
Commit Log中的数据将被清理掉。
SSTables是不可变的，也就是说，当memtable被flush 和保存为一个SSTable文件之后，SSTable不会被再次写入。
因此，一个分区 partition 一般会被保存为多个SSTable文件。所以，如果某一行数据不在memtable里，对这行数据的读写需要遍历所有的SSTable。(后续通过 Compaction 对多个文件进行合并，以提高读写性能。)

## 读请求
读取数据时:
1. 首先检查Bloom filter，每一个SSTable都有一个Bloom filter用来检查partition key是否在这个SSTable，这一步是在访问任何磁盘IO的前面就会做掉。如果存在，再检查partition key cache，然后再做如下操作：

2. 如果在cache中能找到索引，到compression offset map中找拥有这个数据的数据块，从磁盘上取得压缩数据并返回结果集。
3. 如果在cache中找不到索引，搜索partition summary确定索引在磁盘上的大概位置，然后获取索引入口，在SSTable上执行一次单独的寻道和一个顺序的列读取操作，下面也是到compression offset map中找拥有这个数据的数据块，从磁盘上取得压缩数据并返回结果集。

4. 读取数据时会合并Memtable中缓存的数据、多个SSTable中的数据，才返回最终的结果。比如更新用户Email后，用户名、密码等还在老的SSTable中，新的EMail记录到新的SSTable中，返回结果时需要读取新老数据并进行合并。

读请求(Read Request)分两种: 
1. 一种是Rirect Read Request，根据客户端配置的Consistency Level读取到数据即可返回客户端结果。
    1. Consistency Level 为ONE的读取过程，Client连接到任意一个节点上，该节点向实际拥有该数据的节点发出请求，响应最快的节点数据回到Coordinator后，就将数据返回给Client。。
2. 一种是Background Read Repair Request,除了直接请求到达的节点外，会被发送到其它复制节点，用于修复之前写入有问题的节点，保证数据最终一致性。客户端读取时，Coordinator首先联系Consistency Level定义的节点，如果其它节点数据有问题，Coordinator会将最新的数据发送有问题的节点上，进行数据的修复，这个过程被称作Read Repair 。


## 数据整理（Compaction）
更新/删除/插入操作都会把数据顺序写入到一个新的SSTable，并打上一个时间戳以标明数据的新旧, 造成数据冗余+读性能差，所以需要定期:
合并多个SSTable文件中相同partition key的数据

Compaction时:
1. 将多个SSTable文件中的数据整合到新的SSTable文件中，当旧SSTable上的读请求一完成，会被立即删除，空余出来的空间可以重新利用。
2. 虽然Compcation没有随机的IO访问，但还是一个重量级的操作，一般在后台运行，并通过限制它的吞吐量来控制，`compactionthroughputmbpersec参数可以设置，默认是16M/s。
3. 另外，如果key cache显示整理后的数据是热点数据，操作系统会把它放入到page cache里，以提升性能。

Cassandra内建了两种数据压缩策略：参考https://www.datastax.com/blog/leveled-compaction-apache-cassandra

1. SizeTieredCompactionStrategy适用于写操作更多的情况, 最坏情况需要50%的空闲磁盘
2. LeveledCompactionStrategy适用于读操作更多的情况, 最坏情况需要10%的空闲磁盘. 创建固定大小默认是5M的sstable，最上面一级为L0下面为L1，下面一层是上面一层的10倍大小

## 数据复制和分发
数据分发和复制通常是一起的，数据用表的形式来组织，用主键来识别应该存储到哪些节点上，行的copy称作replica。当一个集群被创建时，至少要指定如下几个配置：Virtual Nodes，Partitioner，Replication Strategy，Snitch。

数据复制策略有两种: 
1. 一种是SimpleStrategy，适合一个数据中心的情况，第一份数据放在Partitioner确定的节点，后面的放在顺时针找到的节点上，它不考虑跨数据中心和机架的复制。
2. 另外一种是NetworkTopologyStargegy，第一份数据和前一种一样，第二份复制的数据放在不同的机架上，每个数据中心可以有不同数据的replicas。

Partitioner策略有三种
1. 默认是Murmur3Partitioner，使用MurmurHash。Cassandra默认使用MurmurHash，这种有更高的性能。
2. RandomPartitioner，使用Md5 Hash。
3. ByteOrderedPartitioner使用数据的字节进行有顺分区。

Snitch用来确定从哪个数据中心和哪个机架上写入或读取数据,有如下几种策略：

1. DynamicSnitch：监控各节点的执行情况，根据节点执行性能自动调节，大部分情况推荐使用这种配置
2. SimpleSnitch：不会考虑数据库和机架的情况，当使用SimpleStategy策略时考虑使用这种情况
3. RackInterringSnitch：考虑数据库中和机架
4. PropertyFileSnitch：用cassandra-topology.properties文件来自定义
4. GossipPropertyFileSnitch:定义一个本地的数据中心和机架，然后使用Gossip协议将这个信息传播到其它节点，对应的配置文件是cassandra-rockdc.properties

## 失败检测和修复（Failure detection and recovery）
Cassandra从Gossip信息中确认某个节点是否可用，避免客户端请求路由到一个不可用的节点，或者执行比较慢的节点，这个通过dynamic snitch可以判断出来。Cassandra不是设定一个固定值来标记失败的节点，而是通过连续的计算单个节点的网络性能、工作量、以及其它条件来确定一个节点是否失败。节点失败的原因可能是硬件故障或者网络中断等，节点的中断通常是短暂的但有时也会持续比较久的时间。节点中断并不意味着这个节点永久不可用了，因此不会永久地从网络环中去除，其它节点会定期通过Gossip协议探测该节点是否恢复正常。如果想永久的去除，可以使用nodetool手工删除。

当节点从中断中恢复过来后，它会缺少最近写入的数据，这部分数据由其它复制节点暂为保存，叫做Hinted Handoff，可以从这里进行自动恢复。但如果节点中断时间超过maxhintwindowinms（默认3小时）设定的值，这部分数据将会被丢弃，此时需要用nodetool repair在所有节点上手工执行数据修复，以保证数据的一致性。

## 动态扩展（consistent hashing)
Cassandra最初版本是通过一致性Hash来实现节点的动态扩展的，这会造成数据不均匀。
从1.2版本开始，Cassandra引入了虚拟节点(Virtual Nodes)的概念，Cassandra使用一致性哈希（consistent hashing)

# Reference
1. Cassandra 原理介绍  https://www.cnblogs.com/logo-fox/p/8927067.html
