---
title: 提示词
date: 2023-12-24
private: true
---
Refer: https://datawhalechina.github.io/llm-cookbook/#/C1/2.%20%E6%8F%90%E7%A4%BA%E5%8E%9F%E5%88%99%20Guidelines

# 原则一 编写清晰、具体的指令

## 1.1 使用分隔符清晰地表示输入的不同部分
## 1.2 寻求结构化的输出
## 1.3 要求模型检查是否满足条件
## 1.4 提供少量示例
## 1.5 提示llm 回答的目标群体
编写一个产品描述。
该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。

# 二、原则二 给模型时间去思考

## 2.1 指定完成任务所需的步骤
我们将Prompt加以改进，该 Prompt 前半部分不变，同时确切指定了输出的格式。

prompt_2 = f"""
1-用一句话概括下面用<>括起来的文本。
2-将摘要翻译成英语。
3-在英语摘要中列出每个名称。
4-输出一个 JSON 对象，其中包含以下键：English_summary，num_names。

请使用以下格式：
文本：<要总结的文本>
摘要：<摘要>
翻译：<摘要的翻译>
名称：<英语摘要中的名称列表>
输出 JSON：<带有 English_summary 和 num_names 的 JSON>

Text: <{text}>
"""

## 2.2 指导模型在下结论之前找出一个自己的解法
接下来我们会给出一个问题和一份来自学生的解答，要求模型判断解答是否正确：(增加步骤，要求先解决再对比评估)

prompt = f"""
请判断学生的解决方案是否正确，请通过如下步骤解决这个问题：

步骤：

    首先，自己解决问题。
    然后将您的解决方案与学生的解决方案进行比较，对比计算得到的总费用与学生计算的总费用是否一致，并评估学生的解决方案是否正确。
    **在自己完成问题之前，请勿决定学生的解决方案是否正确。**

使用以下格式：

    问题：问题文本
    学生的解决方案：学生的解决方案文本
    实际解决方案和步骤：实际解决方案和步骤文本
    学生计算的总费用：学生计算得到的总费用
    实际计算的总费用：实际计算出的总费用
    学生计算的费用和实际计算的费用是否相同：是或否
    学生的解决方案和实际解决方案是否相同：是或否
    学生的成绩：正确或不正确

问题：

    我正在建造一个太阳能发电站，需要帮助计算财务。 
    - 土地费用为每平方英尺100美元
    - 我可以以每平方英尺250美元的价格购买太阳能电池板
    - 我已经谈判好了维护合同，每年需要支付固定的10万美元，并额外支付每平方英尺10美元;

    作为平方英尺数的函数，首年运营的总费用是多少。

学生的解决方案：

    设x为发电站的大小，单位为平方英尺。
    费用：
    1. 土地费用：100x美元
    2. 太阳能电池板费用：250x美元
    3. 维护费用：100,000+100x=10万美元+10x美元
    总费用：100x美元+250x美元+10万美元+100x美元=450x+10万美元

实际解决方案和步骤：
"""
# 参数
## 温度
    # 如果需要可预测、可靠的输出，则将 temperature 设置为0。越高则越发散\随机
    response = get_completion(prompt, temperature=0.7)

## role
role 分类：

    system: 系统角色用于指示与对话管理或设置相关的消息。例如，它可能会指定特定的行为、模式或条件，如指导助手使用某种风格或语言进行交流。
    user: 用户角色代表了真实用户，也就是与助手交谈的人。这些消息通常是请求、问题或是对助手回答的反馈。
    assistant: 助手角色指的是GPT或其他AI助手本身。它是对用户输入的响应，包括信息、服务、建议、笑话等。

比如：

    messages =  [  
        {'role':'system', 'content':'你是一个像莎士比亚一样说话的助手。'},    
        {'role':'user', 'content':'给我讲个笑话'},   
        {'role':'assistant', 'content':'鸡为什么过马路'},   
        {'role':'user', 'content':'我不知道'}  
    ]

    def get_completion_from_messages(messages, model="gpt-3.5-turbo", temperature=0):
        response = openai.ChatCompletion.create(
            model=model,
            messages=messages,
            temperature=temperature, # 控制模型输出的随机程度
        )
        return response.choices[0].message["content"]

    response = get_completion_from_messages(messages, temperature=1)
    print(response)
