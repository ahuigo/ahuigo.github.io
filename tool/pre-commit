#!/usr/bin/env python3
import sys
import os
import re
from subprocess import getoutput
from datetime import datetime
from collections import OrderedDict
ROOT = '/b'

errors = []

log = print

def basename(path):
    filename = os.path.split(path)[1]
    return os.path.splitext(filename)[0]

def getPostPath(path):
    return f'{ROOT[1:]}/{path[5:-3]}.html'

def getPostUrl(path):
    return f'/p/{path[5:-3]}'

def updateAtom(blogs, max_items = 10):
    from copy import copy
    from bs4 import BeautifulSoup as BS
    atomPath = f'{ROOT[1:]}/atom.xml'
    if os.path.exists(atomPath):
        s = open(atomPath).read()
    else:
        s = f'''
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<title>Ahuigo 的网络日志</title>
<link href="https://ahuigo.github.io{ROOT}/atom.xml" rel="self" type="application/atom+xml"/>
<updated>2018-08-17T13:46:49Z</updated>
<subtitle>ahuigo's Blog</subtitle>
<entry xml:base="https://ahuigo.github.io{ROOT}/">
<title>Js Array </title>
<link href="#/post/ria/js-array.md" rel="alternate" type="text/html"/>
<author>
<name>ahuigo</name>
<uri>https://ahuigo.github.io{ROOT}/</uri>
</author>
<published>2018-08-16T17:22:32Z</published>
<updated>2018-08-16T17:22:32Z</updated>
<summary/>
<content/>
</entry></feed>
        '''
    bs = BS(s, 'lxml-xml')
    bs.updated.string = datetime.now().replace(microsecond=0).isoformat()+'Z'
    ori_entry = copy(bs.entry)
    ori_entry.content.string = ''

    # clear
    l = len(bs.feed.findChildren('entry'))
    if l > max_items:
        for i in bs.feed.findChildren('entry')[max_items:]:
            i.extract()

    # update
    for path, blog in blogs.items():
        href = getPostUrl(path)
        if bs.find('link', href=href):
            entry =  bs.find('link', href=href).parent
        else:
            entry = copy(ori_entry)

        entry.title.string =  blog['title']
        entry.content.string =  ''
        entry.published.string =  blog['date']
        entry.updated.string =  datetime.now().replace(microsecond=0).isoformat()+'Z'
        if not bs.find('link', href=href):
            entry.link['href'] =  href
            bs.entry.insert_before(entry)
    open(atomPath, 'wb').write(bs.encode())

def breakout(errors):
    print('\n'.join(errors))
    quit(1)

current_date = datetime.now().strftime('%Y-%m-%d')
def parseBlog(path, withContent=False):
    blog = {}
    if withContent:
        data = open(path).read()
    else:
        data = open(path).read(200)
    if data.startswith('---\n'):
        pos = data.index('\n---\n', 4)
        blogStr = data[4:pos]
        data = data[pos+5:]
        for line in blogStr.split('\n'):
            k, v = line.split(':', 1)
            blog[k] = v.strip()
    title, content = data.split('\n', 1)
    if 'title' not in blog:
        blog['title'] = title[2:]
    if 'date' not in blog:
        blog['date'] = ''; 
    blog['content'] = content
    blog['updated'] = current_date
    return blog

def publishBlog(path):
    import html
    # post/a/t.md
    post_path = getPostPath(path)
    post_dir = os.path.dirname(post_path)
    if not os.path.exists(post_dir):
        os.mkdir(post_dir)
    print(f'copy {path} {post_path}')
    blog = parseBlog(path, True)
    meta = f'''---
title: {blog['title']}
date: {blog['date']}
updated: {blog['updated']}
---
'''
    md = f'{meta}# {blog["title"]}\n{blog["content"]}';
    render_md = (open('page.html')
            .read()
            .replace(f'<pre id="markdown"></pre>', f'<pre id="markdown" v-pre>{html.escape(md)}</pre>')
            .replace('{ROOT}', ROOT))
    open(post_path,'w').write(render_md)

def publishBlogs(blogs, delete_blogs):
    '''
    发布的博客列表
    '''
    for path in delete_blogs:
        post_path = getPostPath(path)
        if os.path.exists(post_path):
            os.unlink(post_path)
    for path,blog in blogs.items():
        #if blog['date']:
        publishBlog(path)

if 'all' in sys.argv:
    from glob import glob
    for path in glob('post/**/*.md', recursive=True):
        publishBlog(path)
    quit('compile all')

cmd = 'git diff-index --cached --name-status --diff-filter=ACMRD HEAD -- post | grep -vP "\spost/index.md$" '
output = getoutput(cmd).strip()
if output:
    indexFile = 'post/index.md'
    blogMd = open(indexFile)
    index_blogs = OrderedDict()
    max_index_num = 0
    for line in blogMd:
        m = re.match(
            '^- (?P<updated>\S+) \[(?P<title>.+)\]\((?P<path>/p/.+)\)', line)
        if m:
            max_index_num += 1
            if max_index_num > 50:
                break;
            path = m.group('path').replace('/p/','post/', 1)+'.md'
            index_blogs[path] = {
                'title': m.group('title'),
                'path':path,
                'updated': m.group('updated'),
            }

    new_blogs = {}
    delete_blogs = []
    old_index_path_list = list(index_blogs.keys())
    for line in output.split('\n'):
        status, path = line.split('\t')
        if status == 'D':
            index_blogs.pop(path, None)
            delete_blogs.append(path)
            continue

        if not os.stat(path).st_size:
            continue

        blog = parseBlog(path)
        if 'private' in blog:
            continue
        new_blogs[path] = blog
        index_blogs[path] = blog
        if path not in old_index_path_list:
            index_blogs.move_to_end(path, last=False)

    if errors:
        breakout(errors)

    with open(indexFile, 'w') as fp:
        fp.write(f'# 近期日志\n')
        for path, blog in index_blogs.items():
            article = f'- {blog["updated"]} [{blog["title"]}]({getPostUrl(path)}) '
            print(article)
            fp.write(article+'\n')
    getoutput(f'git add {indexFile}')

    if new_blogs:
        updateAtom(new_blogs)
        new_blogs[indexFile] = {
            'title':'近期日志',
            'date':current_date,
            'updated':current_date,
            'content': open(indexFile).read(),
        }
        publishBlogs(new_blogs, delete_blogs)

if os.path.exists('no-commit'):
    print('no-commit')
    quit(1)
