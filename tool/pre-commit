#!/usr/bin/env python3
import sys
import os
import re
from subprocess import getoutput
from datetime import datetime
from collections import OrderedDict
from bs4 import BeautifulSoup as BS

errors = []

def basename(path):
    filename = os.path.split(path)[1]
    return os.path.splitext(filename)[0]

def updateAtom(blogs, max_items = 20):
    from copy import copy
    bs = BS(open('atom.xml').read(), 'lxml-xml')
    bs.updated.string = datetime.now().replace(microsecond=0).isoformat()+'Z'
    ori_entry = copy(bs.entry)
    ori_entry.content.string = ''

    # clear
    l = len(bs.feed.findChildren('entry'))
    if l > max_items:
        for i in bs.feed.findChildren('entry')[max_items:]:
            i.extract()

    # update
    for path, blog in blogs.items():
        href = '#/'+path
        if bs.find('link', href=href):
            entry =  bs.find('link', href=href).parent
        else:
            entry = copy(ori_entry)

        entry.title.string =  blog['title']
        entry.content.string =  blog['content']
        entry.published.string =  blog['date']
        entry.updated.string =  datetime.now().replace(microsecond=0).isoformat()+'Z'
        if not bs.find('link', href=href):
            entry.link['href'] =  href
            bs.entry.insert_before(entry)
    open('atom.xml', 'wb').write(bs.encode())


def breakout(errors):
    print('\n'.join(errors))
    quit(1)

def parseBlog(path):
    blog = {}
    data = open(path).read(500)
    if data.startswith('---\n'):
        pos = data.index('\n---\n', 4)
        blogStr = data[4:pos]
        data = data[pos+5:]
        for line in blogStr.split('\n'):
            k, v = line.split(':', 1)
            blog[k] = v.strip()
    title, content = data.split('\n', 1)
    if 'title' not in blog:
        blog['title'] = title[2:]
    if 'date' not in blog:
        blog['date'] = ''; #datetime.now().strftime('%Y-%m-%d')
    blog['content'] = content
    return blog

def publishBlog(blogs, delete_blogs):
    import shutil
    #'post/git/git-hooks-rold.md'
    for path in delete_blogs:
        pageFileName = path[5:]
        #os.unlink(f'blog/_posts/{pageFileName}')
    for path,blog in blogs.items():
        if blog['date']:
            tpath = path[5:]
            paths = tpath.split('/', 1)
            if len(paths) > 1 and paths[1].startswith(paths[0]):
                tpath = paths[1]
            pageFilePath = f'blog/_posts/{blog["date"]}-{tpath.replace("/","-")}'
            print(f'copy {path} {pageFilePath}')
            shutil.copy2(path, pageFilePath)
    
cmd = 'git diff-index --cached --name-status --diff-filter=ACMRD HEAD -- post | grep -vP "\spost/0\.md$" '
output = getoutput(cmd).strip()
if output:
    indexFile = 'post/0.md'
    blogMd = open(indexFile)
    blogs = OrderedDict()
    for line in blogMd:
        m = re.match(
            '^- \[(?P<title>.+)\]\(#/(?P<path>.+md)\)', line)
        if m:
            title = m.group('title')
            path = m.group('path')
            blogs[path] = {'title':title, 'path':path,}

    new_blogs = {}
    delete_blogs = []
    for line in output.split('\n'):
        status, path = line.split('\t')
        if status == 'D':
            blogs.pop(path, None)
            delete_blogs.append(path)
            continue

        if not os.stat(path).st_size:
            continue

        blog = parseBlog(path)
        title = blog['title']
        new_blogs[path] = (blog)
        if path not in blogs:
            blogs[path] = blog
            blogs.move_to_end(path, last=False)
        else:
            blogs[path] = blog

    if errors:
        breakout(errors)

    if new_blogs:
        #updateAtom(new_blogs)
        publishBlog(new_blogs, delete_blogs)

    with open(indexFile, 'w') as fp:
        fp.write(f'# 目录\n')
        for path, blog in blogs.items():
            article = f'- [{blog["title"]}](#/{path}) '
            print(article)
            fp.write(article+'\n')
    getoutput(f'git add {indexFile} atom.xml')
