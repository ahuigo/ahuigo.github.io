#!/usr/bin/env python3
# This script is used to generate a static site.
import sys
import os
import re
from subprocess import getoutput
from datetime import datetime
from collections import OrderedDict
from glob import glob

print("Exec python:",sys.executable)
print("current_path:", os.getcwd())

ROOT = "/b"
ROOT = ""
PAGE_BASE="/b"
WWW_PATH = os.getenv('HOME')+'/www'
errors = []
log = print


def basename(path):
    filename = os.path.split(path)[1]
    return os.path.splitext(filename)[0]


# via: /post/c/c-shell.md
def getPostUrl(path):
    return f"{PAGE_BASE}/{path[6:-3]}"


def updateAtom(blogs, max_items=40):
    from copy import copy
    from bs4 import BeautifulSoup as BS

    atomPath = f".{ROOT}/atom.xml"
    if os.path.exists(atomPath):
        s = open(atomPath).read()
    else:
        s = f"""
<feed xmlns="http://www.w3.org/2005/Atom">
<title>Ahuigo 的笔记</title>
<link href="https://ahuigo.github.io{ROOT}/atom.xml" rel="self" type="application/atom+xml"/>
<updated>2018-08-17T13:46:49Z</updated>
<subtitle>Ahuigo's notes</subtitle>
<entry xml:base="https://ahuigo.github.io">
<title>vue instance</title>
<link href="/b/ria/vue-ins" rel="alternate" type="text/html"/>
<updated>2018-09-30T20:29:15Z</updated>
<summary/>
<published/>
<content/>
</entry></feed>
        """
    bs = BS(s, "lxml-xml")
    bs.updated.string = datetime.now().replace(microsecond=0).isoformat() + "Z"
    ori_entry = copy(bs.entry)
    ori_entry.content.string = ""

    # update
    for path, blog in blogs.items():
        href = getPostUrl(path)
        if bs.find("link", href=href):
            entry = bs.find("link", href=href).parent
        else:
            entry = copy(ori_entry)
            entry.link["href"] = href
            bs.entry.insert_before(entry)

        entry.title.string = blog["title"]
        entry.content.string = blog["content"][:100]
        entry.published.string = (
            datetime.strptime(blog["date"], "%Y-%m-%d")
            .replace(microsecond=0)
            .isoformat()
            + "Z"
        )
        entry.updated.string = (
            datetime.strptime(blog["updated"], "%Y-%m-%d")
            .replace(microsecond=0)
            .isoformat()
            + "Z"
        )

    # clear
    l = len(bs.feed.findChildren("entry"))
    if l > max_items:
        for i in bs.feed.findChildren("entry")[max_items:]:
            i.extract()

    open(atomPath, "wb").write(bs.encode())


def breakout(errors):
    log("\n".join(errors))
    quit(1)


current_date = datetime.now().strftime("%Y-%m-%d")


def parseBlog(path, withContent=False):
    blog = {"path": path}
    print(path)
    if withContent:
        data = open(path).read()
    else:
        data = open(path).read(200)
    if data.startswith("---\n"):
        pos = data.index("\n---\n", 4)
        blogStr = data[4:pos]
        data = data[pos + 5 :]
        for line in blogStr.split("\n"):
            k, v = line.split(":", 1)
            blog[k] = v.strip()
    if "title" not in blog:
        blog["title"] = data.split("\n", 1)[0][2:].strip()
    if not blog["title"]:
        quit(f"{path} has no title")
    if "date" not in blog:
        blog["date"] = current_date
    blog["content"] = data
    blog["updated"] = current_date
    return blog


def genPager(pos, max_page):
    pager = ""
    for i in range(max_page):
        page_url = f''
        page_url = "/" if i == 0 else f"/b/p/p{i}"
        pager += f' [p{i}]({page_url})' if i != pos else f" p{i}"
    return pager


def genIndexPages(index_blogs):
    files_num = len(index_blogs)
    SIZE = 20
    page_urls = []
    if not os.path.exists('post/p'):
        os.mkdir("post/p")

    from itertools import islice
    from itertools import groupby, count
    def chunks(iterable, size):
        c = count()
        for i, g in groupby(iterable, lambda _: next(c)//size):
            yield i,g


    it_blogs = iter(index_blogs)
    max_page = -(-len(index_blogs)//SIZE)
    for n,p_blogs in chunks(it_blogs, SIZE):
        page_file = "post/index.md" if n == 0 else f"post/p/p{n}.md"
        print('generate file:',page_file)
        pager = genPager(n, max_page)
        with open(page_file, "w") as fp:
            fp.write(f"# AHUIGO 的笔记\n")
            for path in p_blogs:
                print('pppp:',path)
                blog = index_blogs[path]
                article = f'- {blog["date"]} [{blog["title"]}]({getPostUrl(path)}) '
                fp.write(article + "\n")
            fp.write("\n"+pager)
        print('\n')

    getoutput(f"git add atom.xml post/index.md post/p")


def getIndexBlogs():
    index_blogs = OrderedDict()
    for index_path in ["post/index.md"] + sorted(
        glob("post/p/*.md"), key=lambda x: int(x[8:-3])
    ):
        for line in open(index_path):
            m = re.match(
                "^- (?P<created>\S+) \[(?P<title>[^\]]+)\]\((?P<path>/[^\)]+)\)", line
            )
            if m and m.group("path").startswith(f"{PAGE_BASE}/"):
                path = m.group("path").replace(f"{PAGE_BASE}/", "/post/", 1) + ".md"
                index_blogs[path] = {
                    "title": m.group("title"),
                    "path": path,
                    "date": m.group("created"),
                }
    return index_blogs


if "all" in sys.argv:
    modified_linelist = []
    for path in glob("post/**/*.md", recursive=True):
        if not path.startswith("post/p/") and path != "post/index.md":
            modified_linelist.append(f"A\t{path}")
elif len(sys.argv) > 1:
    modified_linelist = [f"A\t{path}" for path in sys.argv[1:]]
else:
    cmd = 'git diff-index --cached --name-status --diff-filter=ACMRD HEAD -- post | grep -vE "\spost/(index|p/.+).md$" '
    modified_linelist = getoutput(cmd).strip().split("\n")
    # modified_linelist = filter(lambda x: x, modified_linelist)

if modified_linelist:
    index_blogs = getIndexBlogs()
    new_blogs = {}
    delete_files = []
    old_index_path_list = list(index_blogs.keys())
    for line in [line for line in modified_linelist if line]:
        status, path = line.split("\t")
        if status == "D":
            index_blogs.pop(path, None)
            delete_files.append(path)
            continue

        if not os.stat(path).st_size:
            continue

        blog = parseBlog(path)
        if "private" in blog:
            continue
        new_blogs[path] = blog
        index_blogs[path] = blog
        # 'priority' in blog:
        if path not in old_index_path_list:
            index_blogs.move_to_end(path, last=False)

    # breakout(index_blogs)
    if new_blogs or delete_files:
        from itertools import islice
        log("updateAtom, current_path:", os.getcwd())
        updateAtom({k: new_blogs[k] for k in islice(new_blogs, 10)})

        log("genIndexPages")
        genIndexPages(index_blogs)

    if errors:
        breakout(errors)

if os.path.exists("no-commit"):
    log("no-commit")
    quit(1)
